{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d1a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f306a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our base url\n",
    "URL=\"https://www.metacritic.com/browse/games/score/metascore/all/all/filtered\"\n",
    "# Solution for getting 403 forbidden response - send a fake user-agent with every request\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0b187",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Problems that were discovered later on:\n",
    "We had to come back here to take care of them\n",
    "\n",
    "### <font color='red'>Problem1:</font> [Incomplete data](#Incomplete_Data)\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a739816",
   "metadata": {},
   "source": [
    "### Scraping all href links in 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3aad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrefs_list(url):\n",
    "\n",
    "    result = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "\n",
    "    list_wrapper = soup.find_all(\"div\", class_=\"browse_list_wrapper\")\n",
    "\n",
    "    links_list = []\n",
    "    for wrapper in list_wrapper:\n",
    "        for link in wrapper.find_all(\"a\", href = True, class_=\"title\"):\n",
    "            links_list.append(link['href'])\n",
    "    \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3da72",
   "metadata": {},
   "source": [
    "### This function is for getting an href links from a given pages range\n",
    "Every url for each page is in the same structure: 'BASE URL?#num of page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906d3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrefs_list_fromPage_toPage(url, begin = 0, end = 0):\n",
    "    href_list = []\n",
    "    for page in range(begin, end + 1):\n",
    "        if(page != 0):\n",
    "            print(\"page: \",page)\n",
    "            new_url = f\"{url}?page={page}\"\n",
    "        else:\n",
    "            new_url = url\n",
    "        href_list.extend(get_hrefs_list(new_url))\n",
    "    return href_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187970fc",
   "metadata": {},
   "source": [
    "## Start cralwer for the 4000 best games of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb379fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_games_href_list = get_hrefs_list_fromPage_toPage(URL, 0, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a6b03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_games_href_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2f4c4",
   "metadata": {},
   "source": [
    "## Converting the list to dataframe and saving in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ff9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = {'links': best_games_href_list}\n",
    "links_df = pd.DataFrame(data=links_dict)\n",
    "links_df.to_csv('hrefs_csv/good_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffabdc",
   "metadata": {},
   "source": [
    "## Start cralwer for the 4000 worst games of all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa8e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_games_href_list = get_hrefs_list_fromPage_toPage(URL, 159, 199)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2828e",
   "metadata": {},
   "source": [
    "For balance we had to remove 45 first games(from the worst games section) in order to get 4000 games exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f018c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_games_href_list = worst_games_href_list[45:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc31573",
   "metadata": {},
   "source": [
    "## Converting the list to dataframe and saving in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6865a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = {'links': worst_games_href_list}\n",
    "links_df = pd.DataFrame(data=links_dict)\n",
    "links_df.to_csv('hrefs_csv/bad_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a5d11",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec0278",
   "metadata": {},
   "source": [
    "-------------------\n",
    "<a id='Incomplete_Data'></a>\n",
    "### <font color='red'>Problem1:</font>\n",
    "## At the EDA stage we discovered that the data collection was not carried out properly:\n",
    "See problem: [4_EDA](./4_EDA.ipynb)\n",
    "\n",
    "The 'meta_scroe' column is not normally distributed\n",
    "\n",
    "We are missing in 'meta_scroe' the rating between 62 to 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ffa33",
   "metadata": {},
   "source": [
    "### We decided to collect ALL the data that we can get from the website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cb033",
   "metadata": {},
   "source": [
    "### Start cralwer for ALL the games hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28643766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:  1\n",
      "page:  2\n",
      "page:  3\n",
      "page:  4\n",
      "page:  5\n",
      "page:  6\n",
      "page:  7\n",
      "page:  8\n",
      "page:  9\n",
      "page:  10\n",
      "page:  11\n",
      "page:  12\n",
      "page:  13\n",
      "page:  14\n",
      "page:  15\n",
      "page:  16\n",
      "page:  17\n",
      "page:  18\n",
      "page:  19\n",
      "page:  20\n",
      "page:  21\n",
      "page:  22\n",
      "page:  23\n",
      "page:  24\n",
      "page:  25\n",
      "page:  26\n",
      "page:  27\n",
      "page:  28\n",
      "page:  29\n",
      "page:  30\n",
      "page:  31\n",
      "page:  32\n",
      "page:  33\n",
      "page:  34\n",
      "page:  35\n",
      "page:  36\n",
      "page:  37\n",
      "page:  38\n",
      "page:  39\n",
      "page:  40\n",
      "page:  41\n",
      "page:  42\n",
      "page:  43\n",
      "page:  44\n",
      "page:  45\n",
      "page:  46\n",
      "page:  47\n",
      "page:  48\n",
      "page:  49\n",
      "page:  50\n",
      "page:  51\n",
      "page:  52\n",
      "page:  53\n",
      "page:  54\n",
      "page:  55\n",
      "page:  56\n",
      "page:  57\n",
      "page:  58\n",
      "page:  59\n",
      "page:  60\n",
      "page:  61\n",
      "page:  62\n",
      "page:  63\n",
      "page:  64\n",
      "page:  65\n",
      "page:  66\n",
      "page:  67\n",
      "page:  68\n",
      "page:  69\n",
      "page:  70\n",
      "page:  71\n",
      "page:  72\n",
      "page:  73\n",
      "page:  74\n",
      "page:  75\n",
      "page:  76\n",
      "page:  77\n",
      "page:  78\n",
      "page:  79\n",
      "page:  80\n",
      "page:  81\n",
      "page:  82\n",
      "page:  83\n",
      "page:  84\n",
      "page:  85\n",
      "page:  86\n",
      "page:  87\n",
      "page:  88\n",
      "page:  89\n",
      "page:  90\n",
      "page:  91\n",
      "page:  92\n",
      "page:  93\n",
      "page:  94\n",
      "page:  95\n",
      "page:  96\n",
      "page:  97\n",
      "page:  98\n",
      "page:  99\n",
      "page:  100\n",
      "page:  101\n",
      "page:  102\n",
      "page:  103\n",
      "page:  104\n",
      "page:  105\n",
      "page:  106\n",
      "page:  107\n",
      "page:  108\n",
      "page:  109\n",
      "page:  110\n",
      "page:  111\n",
      "page:  112\n",
      "page:  113\n",
      "page:  114\n",
      "page:  115\n",
      "page:  116\n",
      "page:  117\n",
      "page:  118\n",
      "page:  119\n",
      "page:  120\n",
      "page:  121\n",
      "page:  122\n",
      "page:  123\n",
      "page:  124\n",
      "page:  125\n",
      "page:  126\n",
      "page:  127\n",
      "page:  128\n",
      "page:  129\n",
      "page:  130\n",
      "page:  131\n",
      "page:  132\n",
      "page:  133\n",
      "page:  134\n",
      "page:  135\n",
      "page:  136\n",
      "page:  137\n",
      "page:  138\n",
      "page:  139\n",
      "page:  140\n",
      "page:  141\n",
      "page:  142\n",
      "page:  143\n",
      "page:  144\n",
      "page:  145\n",
      "page:  146\n",
      "page:  147\n",
      "page:  148\n",
      "page:  149\n",
      "page:  150\n",
      "page:  151\n",
      "page:  152\n",
      "page:  153\n",
      "page:  154\n",
      "page:  155\n",
      "page:  156\n",
      "page:  157\n",
      "page:  158\n",
      "page:  159\n",
      "page:  160\n",
      "page:  161\n",
      "page:  162\n",
      "page:  163\n",
      "page:  164\n",
      "page:  165\n",
      "page:  166\n",
      "page:  167\n",
      "page:  168\n",
      "page:  169\n",
      "page:  170\n",
      "page:  171\n",
      "page:  172\n",
      "page:  173\n",
      "page:  174\n",
      "page:  175\n",
      "page:  176\n",
      "page:  177\n",
      "page:  178\n",
      "page:  179\n",
      "page:  180\n",
      "page:  181\n",
      "page:  182\n",
      "page:  183\n",
      "page:  184\n",
      "page:  185\n",
      "page:  186\n",
      "page:  187\n",
      "page:  188\n",
      "page:  189\n",
      "page:  190\n",
      "page:  191\n",
      "page:  192\n",
      "page:  193\n",
      "page:  194\n",
      "page:  195\n",
      "page:  196\n",
      "page:  197\n",
      "page:  198\n",
      "page:  199\n",
      "page:  200\n"
     ]
    }
   ],
   "source": [
    "all_href_list = get_hrefs_list_fromPage_toPage(URL, 0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d137338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_href_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6ed5b",
   "metadata": {},
   "source": [
    "## Converting the list to dataframe and saving in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f126f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = {'links': all_href_list}\n",
    "links_df = pd.DataFrame(data=links_dict)\n",
    "links_df.to_csv('hrefs_csv/all_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba911b55",
   "metadata": {},
   "source": [
    "### We will continue the problem handling in the game crawling stage:\n",
    "\n",
    "See next step: [game_crawling](./2_game_crawling.ipynb) => <font color='red'>'Problem1: Incomplete data'</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
